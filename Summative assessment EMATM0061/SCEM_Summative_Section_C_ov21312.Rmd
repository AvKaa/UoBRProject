---
title: "SCEM Summative Section_C"
author: "Averil Kan (ov21312)"
date: "30/11/2021"
output: html_document
---
# Introduction

The following section demonstrates a Random Forest predictive modelling on the dataset 'winequality-red'. The objective of the model is to 


# Random Forest Introduction

To understand the random forest model, the concept of a decision tree must be realised.

In a dataset the decision tree splits the data recursively using the decision nodes unless it is left with pure leaf nodes. By maximizing the entropy gain to find the best split. If a given data sample satisfies the condition at a decision node it will branch towards the left, else to the right until it reaches a leaf node where a class label is assigned to it. However, decision trees are highly sensitive to the training data which could result in high variance, thus, the model might fail to generalise.

This is where the Random Forest comes in handy, the Random Forest is a collection of multiple random decision trees and is much less sensitive to the training data. When a data set is passed into a Random Forest model, new sub-datasets are built from the original data. Every sub-dataset contains the same number of rows as the original one and each row is randomly sampled with replacement from the original dataset. This process of generating new datasets is known as Bootstrapping. An independent decision tree will then be trained on each bootstrapped sub-datasets with a subset of features that are randomly selected for each tree (not all of the features are used in each tree). After a forest is formed, data points will be passed through each tree one by one and the predictions will be recorded. The predictions will then be aggregated through a 'majority vote' which would result in a classification prediction. Random Forest can also be used for regression problems just by taking the average instead of the 'majority vote' for combined predctions. 


```{r, echo=FALSE}
# #Required libraries 
# install.packages("tidyverse")
# install.packages("corrplot")
# install.packages("randomForest")
# install.packages("ranger")
```

```{r, include=FALSE}
# library(ggplot2, ggthemes, corrplot, reshape2, dplyr, randomForest)
library(tidyverse)
library(corrplot)
library(randomForest)
library(ranger)
library(cowplot)
```

# Red Wine Quality Dataset Introduction
For this project, the Red Wine Quality dataset was used to build various classification models to predict the quality of red wines. A “quality” score between 0 and 10 is given to each wine in this dataset.  The quality of a wine is determined by eleven input variables as following:

- Fixed acidity
- Volatile acidity
- Citric acid
- Residual sugar
- Chlorides
- Free sulfur dioxide
- Total sulfur dioxide
- Density
- pH
- Sulfates
- Alcohol

The summary of the dataset is displayed as following:
```{r, message=FALSE,warning=FALSE}
#Load red wine dataset
redwine<-read.csv("./winequality-red.csv")
redwine <- redwine %>%
  drop_na()
#summary statistics
str(redwine)
summary(redwine)
```

# Correlation of Variables

A correlation matrix was then plotted to obtain a better understanding of the relationships between the variables:
```{r,message=FALSE,warning=FALSE}
# Scatterplot Matrix of Variables
plot(redwine)

# Correlation Heatmap of Variables
corrplot(cor(redwine), method = "square",
         type = "full", tl.cex=0.8,tl.col = "black")
```

As wine quality was the prediction objective, consider the final column/row in order to find out which variable(s) has the strongest relationship with the wine quality. It can be observed from the heat map that alcohol has the strongest correlation with wine quality.

```{r}
# Correlation test
a<-cor.test(redwine$alcohol,redwine$quality)
a
a$p.value
```

# Wine Quality Distribution

The distribution of wine quality was then observed with the following plot:

```{r,message=FALSE,warning=FALSE}
#Distribution of red wine quality ratings
ggplot(redwine,aes(x=quality))+
  geom_bar(stat = "count",position = "dodge")+
  scale_x_continuous(breaks = seq(3,8,1))+
  ggtitle("Distribution of Red Wine Quality")+
  theme_classic()
```

It can be observed that the quality of wine was not distributed evenly, and that wines of quality 1,2,9 and 10 are not present. Moreover, most wine qualities are concentrated at 5 and 6. A split for bad quality wine (1-5) and good quality wine (6-10) will be implemented. However, to test the capability of the Random Forest model on an imbalanced dataset, the dataset is first passed on as it is containing 6 categories of wine quality. This is done because the undersampling/ oversampling of data points are common problems in the field of data mining and machine learning.

# Predictive Modelling 

The dataset was split into the train and test datasets with a split ratio of 0.8. It is important to perform a test split, as the goal of predictive modelling is to observe the accuracy of the model on data points that are not used in training.

```{r, message=FALSE,warning=FALSE}
seed=0
set.seed(seed)

train_ratio <- 0.8 # Define split ratio
num_total <- redwine %>% nrow()
num_train <- floor(num_total*train_ratio)
num_test <- num_total-num_train

test_ind <- sample(seq(num_total),num_test) # Define train indices
train_ind <- setdiff(seq(num_total),test_ind) # Define test indices

# Pass selection of row indices from original dataset to respective subsets
redwine_train <- redwine %>% filter(row_number() %in% train_ind)
redwine_test <- redwine %>% filter(row_number() %in% test_ind)
```

Once the test split has been performed, the train set was passed on to the Random Forest model for training. The test set was then passed on to the model obtained from the training set to compute the model accuracy and the confusion matrix.

```{r, message=FALSE,warning=FALSE}
# RandomForest Model
set.seed(seed) # Set seed for replicability 
redwine_model<-randomForest(factor(quality)~.-quality,redwine_train,ntree = 500, importance = TRUE) # importance of predictors will be assessed. Hence, importance set to TRUE

# Predicting on train set
pred_train <- predict(redwine_model, redwine_train, type='class')
# Checking classification accuracy
cm_train <- table(pred_train, redwine_train$quality) # Confusion matrix
accuracy_train <- mean(pred_train == redwine_train$quality)                    

# Predicting on test set
pred_test <- predict(redwine_model, redwine_test, type='class')
# Checking classification accuracy
accuracy_test <- mean(pred_test == redwine_test$quality)                    
cm_test <- table(pred_test,redwine_test$quality)

accuracy_train
accuracy_test
cm_test
```

The overall accuracy of the model is around 73%, which is decent considering only the baseline model of Random Forest was implemented. It can also be observed that most of the errors originates from the differentiation of 5 and 6 quality wines. This is predicted as most wines are concentrated at quality 5 and 6, and that the input variables are harder to differentiate due to the quality being one apart.

However, the model demonstrated a clear overfitting behavior as it performs really well on the training set (100% accuracy) and does not perform as well on the test set (73% accuracy). For a better approximation for the accuracy cross validation is then implemented.

# Cross Validation

After the model is trained, assumptions cannot be made on whether the model is going to work well on data that it has not seen before yet. In other words, it is uncertain that the model will have the desired accuracy and variance in the production environment. Hence, an assurance of the accuracy of the predictions that the trained model produced is required. This is done through the validation of the model. The validation process decides whether the numerical results quantifying hypothesised relationships between each variable are acceptable as descriptions of the data.

A K-Folds cross validation technique was then implemented to verify if the accuracy is consistent when different data points are fed in as test data. An advantage of the K-Fold technique is that it ensures that every data points from the original dataset has the chance in participating in the training and the test set.

```{r, message=FALSE,warning=FALSE}
set.seed(seed) # Set seed for replicability 

num_groups = 5  
n <- seq(5)
s <- list()
acc <- c()


row_ind <- sample(nrow(redwine)) # Randomisation of row indices  
redwine_randomized <- redwine[row_ind,] # Form randomised dataset
split <- redwine_randomized %>% # Section the randomised dataset into 5 groups 
   group_by((row_number()-1) %/% (n()/num_groups)) %>%
   nest %>% 
   pull(data)

# Define the indexes of datasets to be parsed on to either the train or test set in for each loop
for (i in n){
  s[length(s)+1]<-list(n[!(n %in% i)])
}

# Run the randomForest function across each k-fold 
for (i in 1:5){
  test_set = split[[i]]
  train_set = data.frame()
  for(j in 1:4){
    index<-s[[i]][j]
    train_set <- rbind(train_set,split[[index]])
  }
  redwine_model<-randomForest(factor(quality)~.-quality,train_set,ntree = 500, mtry = 6)
  pred_test <- predict(redwine_model, test_set, type='class')
  # Checking classification accuracy
  acc[i] <- mean(pred_test == test_set$quality)
}

# Compute average accuracy 
mean(acc)
```

The cross validation resulted in an average accuracy of around 70% which is around 3% lower than the single test split Random Forest model. This is unexpected as cross-validation tends to result in less overfitting, this might be the result of the seed selection. However, the key of performing cross validation on the model is to provide a more realistic proxy for the accuracy. Thus, the accuracy of the model is accepted as approximately 70%.

# Variable Importance

For each tree, the prediction error on the out-of-bag (OOB) portion of the data is recorded. In the case of classification this is the error rate for classification (MSE for regression). Then the same is done after permuting each predictor variable. The difference between the two are then averaged over all trees and normalized by the standard deviation of the differences. The importance function is part of the Random Forest package.

The respective variable importance can be observed with the following plot: 

```{r, message=FALSE,warning=FALSE}
# Get importance
importance    <- randomForest::importance(redwine_model)

var_Importance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rank_Importance <- var_Importance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

# Use ggplot2 to visualize the relative importance of variables
ggplot(rank_Importance, aes(x = reorder(Variables, Importance), 
                           y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
            hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() + 
  theme_bw()
```

The variable importance plot demonstrates that the 'alcohol' level has the greatest impact on the quality of the red wine. This is coherent with the correlation heat map shown above. The correlations between the variables of top 4 importance and the quality of the red wines are then further investigated by the following scatter plots:

```{r, message=FALSE,warning=FALSE}
a_q <- ggplot(redwine, aes(alcohol, quality)) +
  geom_point()+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  theme_bw()
va_q <- ggplot(redwine, aes(volatile.acidity, quality)) +
  geom_point()+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  theme_bw()
s_q <- ggplot(redwine, aes(sulphates, quality)) +
  geom_point()+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  theme_bw()
tsd_q <- ggplot(redwine, aes(total.sulfur.dioxide, quality)) +
  geom_point()+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  theme_bw()


plot_grid(a_q, va_q, s_q, tsd_q, labels = "AUTO")
```


The plots demonstrates that wines with higher quality have higher levels of alcohol, have a lower volatile acidity, higher levels of sulphates, and lower levels of total sulfur dioxide on average.

# Hyperparameter tuning

While the model parameters are learned during the training process, hyperparameters must be set before training. In the case of a Random Forest model, the hyperparameters can include the number of decision tress, the number of features considered when splitting a node by each tree, the node size, sample size etc. The tuning of these parameters relies on a more experimental approach. Therefore, iterations will be carried out in order to search for the best combination(s) of hyperparameters.  

The hyperparameters selected to be tuned for the random forest model are defined as following:
-mtry: The number of variables available for splitting at each tree node. In classification models, the function default is floor of the square root of the number of predictor variables. Which is 3 for the red wine data frame.
-node_size:  The minimum number of observations in a terminal node. A large node size causes smaller trees to be grown, thus, takes less time. The default value for the Random Forest classification model is 1 (5 for regression).
-sample_size = The sample size of a random forest acts as a control of the "degree of randomness", thus, a way of adjusting the bias variance trade-off. The increase in sample size leads to a "less random" forest, which would induce the tendency of an overfitting model. On the other hand decreasing the sample size increases the variation per tree within the forest at the expense of model performance. The sample size ranges from 0 < sample size ≤ 1.
-ntree = The number of trees in the forest, the ntree value is independent of other hyperparameters as the accuracy will converge into a stable value after a certain number of trees. Hence, the convergence ntree value will be computed firstly. It is also important that the greater the value of ntree the greater computational force required.

The accuracy of the baseline Random Forest model across a sequence of ntree values can be observed with the following plot:
```{r, message=FALSE,warning=FALSE}
set.seed(seed)

ntree <- seq(100,1500,by=10)
ntree_acc <- function(ntree){
  redwine_model<-randomForest(factor(quality)~.-quality,redwine_train,ntree = ntree, importance = TRUE)
  pred_test <- predict(redwine_model, redwine_test, type='class')
  accuracy_test <- mean(pred_test == redwine_test$quality)                    

  return(accuracy_test)
}

tune_ntree <- data.frame(ntree=ntree)%>%
  mutate(accuracy = map(.x=ntree,.f=~ntree_acc(.x)))

tune_ntree %>%   
  mutate(accuracy = as.numeric(accuracy)) %>%
  ggplot(aes(ntree,accuracy)) +
  geom_line() +
  geom_smooth(se=F)+
  theme_bw()
```
The accuracy stabalises at around 73.5% after the ntree value reaches 600. Thus, to minimize computational force for hyperparameter tuning, a ntree value of 600 will be used.

```{r, message=FALSE,warning=FALSE}
# hyperparameter grid search
hyper_grid <- expand.grid(
  mtry       = seq(1, 10, by = 2),
  node_size  = seq(1, 5, by = 1),
  sample_size = seq(0.1, 1, by = 0.1),
  acc = 0 # Define empty list to store test accuracies
)
```

Random forest model tuning to obtain the best combinations of hyperparameters shown in the table as following:

```{r, message=FALSE,warning=FALSE}
redwine_train<-redwine_train%>%mutate(quality=as.factor(quality)) # `quality` must be fed into the model as factor

for(i in 1:nrow(hyper_grid)) {

  # Train model
  model <- randomForest(
    formula         = quality~ .,
    data            = redwine_train,
    num.trees       = 600, # Set fix ntree value to 600
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sample_size[i],
    seed            = seed
  )

  # Predicting on test set
  pred_test <-predict(model, redwine_test, type='class')
  # Checking classification accuracy
  accuracy <- mean(pred_test == redwine_test$quality)


  # Add confusion matrix to grid
  hyper_grid$acc[i] <- accuracy
}

hyper_grid %>% # Display the top 10 hyperparameter combination
  dplyr::arrange(desc(acc)) %>%
  head(10)
```
The best combination of hyperparameters results in an accuracy of around 75%, which is 5% above the k-fold cross-validated model. This shows that the tuning of hyperparameters does have a good amount of impact on Random Forest predictive modelling.

The table shows that the best mtry values are 1 and 3, which is in coherent with the Random Forest theory, Where the best mtry value should be the square root of the number of predictor values (the redwine dataset contains 12 predictor values, floor(sqrt(3))=3). It can also be observed that the most suitable node size is 2 with three occurrence and 4 with four occurrence in the table. However, a 'best' sample size value cannot be ascertained, this might the result of heavier dependencies that certain hyperparameters have on other hyperparameters.

# Random Forest on Bianry Classification

As mentioned above, most wine qualities are concentrated at 5 and 6. After testing the capabilities of the Random Forest model on an imbalanced dataset. The quality of wines will be redefined as bad quality wine (1-5) and good quality wine (6-10) to obtain a relatively balanced dataset for  a binary classification prediction model. 

The distribution of good and bad quality wines can be observed with the following plot:
```{r, message=FALSE,warning=FALSE}

redwine<-read.csv("./winequality-red.csv") # Recall redwine.csv as 'quality' was converted as factor above

# Create binary quality column and remove the quality column
redwine$wine_quality<-ifelse(redwine$quality>5,1,0)
redwine <- redwine %>%
  select(!quality)

# Display distribution of good and bad quality wines
ggplot(redwine,aes(x=wine_quality,fill=factor(wine_quality)))+geom_bar(stat = "count",position = "dodge")+
  scale_x_continuous(breaks = seq(0,1,1))+
  ggtitle("Distribution of Good and Bad Red Quality Wines")+
  labs(x="Wine quality",y="Count")+
  theme_classic()
```

Following the same methodology of test splitting, the Random Forest binary classification accuracy for the red wine dataset is obtained as following:
```{r, message=FALSE,warning=FALSE}
# Test split
set.seed(seed)

train_ratio <- 0.8 # Define split ratio
num_total <- redwine %>% nrow()
num_train <- floor(num_total*train_ratio)
num_test <- num_total-num_train
test_ind <- sample(seq(num_total),num_test) # Define train indices
train_ind <- setdiff(seq(num_total),test_ind) # Define test indices

# Pass selection of row indices from original dataset to respective subsets
redwine_train <- redwine %>% filter(row_number() %in% train_ind)
redwine_test <- redwine %>% filter(row_number() %in% test_ind)

# Convert 'wine_quality' to factor
redwine_train<-redwine_train %>%
  mutate(wine_quality=as.factor(wine_quality)) # `quality` must be fed into the model as factor

# RandomForest Model
redwine_model_binary<-randomForest(factor(wine_quality)~.-wine_quality,redwine_train,ntree = 500, importance = TRUE) # importance of predictors will be assessed. Hence, importance set to TRUE

# Predicting on train set
pred_train <- predict(redwine_model_binary, redwine_train, type='class')
# Checking classification accuracy
cm_train <- table(pred_train, redwine_train$wine_quality) # Confusion matrix
accuracy_train <- mean(pred_train == redwine_train$wine_quality)                    

# Predicting on test set
pred_test <- predict(redwine_model_binary, redwine_test, type='class')
# Checking classification accuracy
accuracy_test <- mean(pred_test == redwine_test$wine_quality)                    
cm_test <- table(pred_test,redwine_test$wine_quality)

accuracy_train
accuracy_test
cm_test
```

The overall accuracy of the model is pretty high at around 86%. An overfitting behavior can still be observed, additionally, it can be seen that the model does a better job at predicting good wines than bad wines. A k-fold cross validation is again performed as following:

```{r, message=FALSE,warning=FALSE}
set.seed(seed) # Set seed for replicability 

row_ind <- sample(nrow(redwine)) # Randomisation of row indices  
redwine_randomized <- redwine[row_ind,] # Form randomised dataset
split <- redwine_randomized %>% # Section the randomised dataset into 5 groups 
   group_by((row_number()-1) %/% (n()/num_groups)) %>%
   nest %>% 
   pull(data)

# Define the indexes of datasets to be parsed on to either the train or test set in for each loop
for (i in n){
  s[length(s)+1]<-list(n[!(n %in% i)])
}

# Run the randomForest function across each k-fold 
for (i in 1:5){
  test_set = split[[i]]
  train_set = data.frame()
  for(j in 1:4){
    index<-s[[i]][j]
    train_set <- rbind(train_set,split[[index]])
  }
  redwine_model<-randomForest(factor(wine_quality)~.-wine_quality,train_set,ntree = 500, mtry = 6)
  pred_test <- predict(redwine_model, test_set, type='class')
  # Checking classification accuracy
  acc[i] <- mean(pred_test == test_set$wine_quality)
}

# Compute average accuracy 
mean(acc)
```

The cross validation accuracy is 82% which is also slightly lower than the single split random forest model (by 4%). 

Following the same steps, create a hypergrid and pass it into the Random Forest mdoel for hyperparameter tuning.

```{r, message=FALSE,warning=FALSE}
# hyperparameter grid search
hyper_grid_binary <- expand.grid(
  mtry       = seq(1, 10, by = 2),
  node_size  = seq(1, 5, by = 1),
  sample_size = seq(0.1, 1, by = 0.1),
  acc = 0 # Define empty list to store test accuracies
)
```

```{r}
  # Train model
  model <- randomForest(
    formula         = wine_quality~ .,
    data            = redwine_train,
    num.trees       = 600, # Set fix ntree value to 600
    mtry            = hyper_grid_binary$mtry[1],
    min.node.size   = hyper_grid_binary$node_size[1],
    sample.fraction = hyper_grid_binary$sample_size[1],
    seed            = seed
  )

  # Predicting on test set
  pred_test <-predict(model, redwine_test, type='class')
  # Checking classification accuracy
  accuracy <- mean(pred_test == redwine_test$wine_quality)
accuracy

  # Add confusion matrix to grid
  hyper_grid_binary$acc[i] <- accuracy
```


```{r}
redwine <- redwine %>%
  mutate(wine_quality=as.factor(wine_quality))

for(i in 1:nrow(hyper_grid_binary)) {

  # Train model
  model <- randomForest(
    formula         = wine_quality~ .,
    data            = redwine_train,
    num.trees       = 600, # Set fix ntree value to 600
    mtry            = hyper_grid_binary$mtry[i],
    min.node.size   = hyper_grid_binary$node_size[i],
    sample.fraction = hyper_grid_binary$sample_size[i],
    seed            = seed
  )

  # Predicting on test set
  pred_test <-predict(model, redwine_test, type='class')
  # Checking classification accuracy
  accuracy <- mean(pred_test == redwine_test$wine_quality)


  # Add confusion matrix to grid
  hyper_grid_binary$acc[i] <- accuracy
}

hyper_grid_binary %>% # Display the top 10 hyperparameter combination
  dplyr::arrange(desc(acc)) %>%
  head(10)
```

The best combination of hyperparameters results in an accuracy of around 87%, which is 5% above the k-fold cross-validated model. 

# Random Forest from different packages

In the following section the hyperparameter tuning will be carried out using a d


















