---
title: "SCEM_Summative_Section_C_ov21312"
author: "Avekan"
date: "30/11/2021"
output: html_document
---

# Random Forest Introduction

To understand the random forest model, the concept of a decision tree must be realised.

In a dataset the decision tree splits the data recursively using the decision nodes unless it is left with pure leaf nodes. By maximizing the entropy gain to find the best split. If a given data sample satisfies the condition at a decision node it will branch towards the left, else to the right until it reaches a leaf node where a class label is assigned to it. However, decision trees are highly sensitive to the training data which could result in high variance, thus, the model might fail to generalise.

This is where the Random Forest comes in handy, the Random Forest is a collection of multiple random decision trees and is much less sensitive to the training data. When a data set is passed into a Random Forest model, new sub-datasets are built from the original data. Every sub-dataset contains the same number of rows as the original one and each row is randomly sampled with replacement from the original dataset. This process of generating new datasets is known as Bootstrapping. An independent decision tree will then be trained on each bootstrapped sub-datasets with a subset of features that are randomly selected for each tree (not all of the features are used in each tree). After a forest is formed, data points will be passed through each tree one by one and the predictions will be recorded. The predictions will then be aggregated through a 'majority vote' which would result in a classification prediction. Random Forest can also be used for regression problems just by taking the average instead of the 'majority vote' for combined predctions. 


```{r, echo=FALSE}
# #Libraries needed
# install.packages("tidyverse")
# install.packages("corrplot")
# install.packages("randomForest")
# install.packages("ranger")
```

```{r, echo=FALSE}
# library(ggplot2, ggthemes, corrplot, reshape2, dplyr, randomForest)
library(tidyverse)
library(corrplot)
library(randomForest)
library(ranger)
```

# RedWine Quality dataset Introduction
For this project, Kaggle’s Red Wine Quality dataset was used to build a various classification models to predict the quality of red wines. A “quality” score between 0 and 10 is given to each wine in this dataset.  The quality of a wine is determined by eleven input variables as following:

- Fixed acidity
- Volatile acidity
- Citric acid
- Residual sugar
- Chlorides
- Free sulfur dioxide
- Total sulfur dioxide
- Density
- pH
- Sulfates
- Alcohol

The summary of the dataset is displayed as following:
```{r}
#Load red wine dataset
redwine<-read.csv("./winequality-red.csv")
redwine <- redwine %>%
  drop_na()
#summary statistics
str(redwine)
summary(redwine)
```

# Correlation of Variables

A correlation matrix was then plotted to obtain a better understanding of the relationships between the variables:
```{r,message=FALSE,warning=FALSE}
#Scatterplot Matrix of Variables
plot(redwine)

#Correlation Heatmap of Variables
corrplot(cor(redwine))
```

As wine quality was the prediction objective, the final column/row was considered in order to know which variables has the strongest relationship with the wine quality. It can be observed from the heat map that alcohol has the strongest correlation with wine quality.

# Wine Quality Distribution

The distribution of wine quality was then observed with the following plot:
```{r,message=FALSE,warning=FALSE}
#Distribution of red wine quality ratings
ggplot(redwine,aes(x=quality))+geom_bar(stat = "count",position = "dodge")+
  scale_x_continuous(breaks = seq(3,8,1))+
  ggtitle("Distribution of Red Wine Quality")+
  theme_classic()

```

It can be observed that the quality of wine was not distributed evenly, and that wines of quality 1,2,9 and 10 are not present. Moreover, most wine qualities are concentrated at 5 and 6. A split for bad quality wine (1-5) and good quality wine (6-10) will be implemented. However, to test the extent of the Random Forest model, the dataset was passed on as it is with 6 categories of wine quality.

# Predictive Modelling (Binary Classification)

The dataset was split into the train and test datasets with a split ratio of 0.8.
```{r}
set.seed(0)

train_ratio <- 0.8
num_total <- redwine %>% nrow()
num_train <- floor(num_total*train_ratio)
num_test <- num_total-num_train

test_ind <- sample(seq(num_total),num_test)
train_ind <- setdiff(seq(num_total),test_ind)

redwine_train <- redwine %>% filter(row_number() %in% train_ind)
redwine_test <- redwine %>% filter(row_number() %in% test_ind)
```


Once the test split has been performed, the train set was passed on to the Random Forest model for training. The test set was then passed on to the model obtained from the training set to obtain the accuracy and the confusion matrix.
```{r}
# RandomForest Model
set.seed(0) # Set seed for replicability 
redwine_model<-randomForest(factor(quality)~.-quality,redwine_train,ntree = 500, importance = TRUE) # importance of predictors will be assessed. Hence, importance set to TRUE

# Predicting on train set
pred_train <- predict(redwine_model, redwine_train, type='class')
# Checking classification accuracy
cm_train <- table(pred_train, redwine_train$quality) # Confusion matrix

# Predicting on test set
pred_test <- predict(redwine_model, redwine_test, type='class')
# Checking classification accuracy
accuracy <- mean(pred_test == redwine_test$quality)                    
cm_test <- table(pred_test,redwine_test$quality)


accuracy
cm_test
```

The overall accuracy of the model is around 73%, which is decent considering only the baseline model of Random Forest was implemented. It can also be observed that most of the errors originates from the differentiation of 5 and 6 quality wines. This is predicted as most wines are concentrated at quality 5 and 6, and that the input variables are harder to differentiate due to the quality being one apart.

# Cross Validation

A cross validation was then implemented to 
```{r}
rf_cv <- rfcv(trainx = redwine,
         trainy = redwine$quality,
         cv.fold=5)
with(rf_cv, plot(n.var, error.cv, log="x", type="o", lwd=2))

result <- replicate(5, rfcv(redwine, redwine$quality), simplify=FALSE)
error.cv <- sapply(result, "[[", "error.cv")
matplot(result[[1]]$n.var, cbind(rowMeans(error.cv), error.cv), type="l",
        lwd=c(2, rep(1, ncol(error.cv))), col=1, lty=1, log="x",
        xlab="Number of variables", ylab="CV Error")

```


### Variable Importance

```{r,message=FALSE,warning=FALSE}
# Get importance
importance    <- randomForest::importance(redwine_model)

var_Importance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rank_Importance <- var_Importance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

# Use ggplot2 to visualize the relative importance of variables
ggplot(rank_Importance, aes(x = reorder(Variables, Importance), 
                           y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
            hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() + 
  theme_bw()
```

###Hyperparameter tuning
Define hyperparameters to tune for the random forest model. In this case the chosen parameters are:
mtry = 
node_size = 
sample_size =
```{r}
# hyperparameter grid search
hyper_grid <- expand.grid(
  mtry       = seq(1, 10, by = 2),
  node_size  = seq(1, 5, by = 1),
  sample_size = seq(0.5, 1, by = 0.1),
  acc   = 0
)
```


Tuning of random forest model with use of the ranger package
```{r}
redwine_train<-redwine_train%>%mutate(quality=as.factor(quality)) # `quality` must be as factor

for(i in 1:nrow(hyper_grid)) {

  # train model
  model <- randomForest(
    formula         = quality~ .,
    data            = redwine_train,
    num.trees       = 500,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sample_size[i],
    seed            = 0
  )

  # Predicting on test set
  pred_test <-predict(model, redwine_test, type='class')
  # Checking classification accuracy
  accuracy <- mean(pred_test == redwine_test$quality)  

  
  # add confusion matrix to grid
  hyper_grid$acc[i] <- accuracy
}

hyper_grid %>%
  dplyr::arrange(desc(acc)) %>%
  head(10)
```


```{r}
#Load red wine dataset
redwine<-read.csv("./winequality-red.csv")
#Create a variable indicating if a wine is good or bad

quality_2_class<-function(quality){
  return(if(quality<5){
    as.integer(0)
  }else if(quality>6){
    as.integer(2)
  }else{
    as.integer(1)
  })
}


redwine$quality_wine<-map_dbl(.x=redwine$quality,.f=~quality_2_class(.x))
  

#Distribution of good/bad red wines
ggplot(redwine,aes(x=quality_wine,fill=factor(quality_wine)))+geom_bar(stat = "count",position = "dodge")+
  scale_x_continuous()+
  ggtitle("Distribution of Good/Mediocre/Bad Red Wines")+
  theme_classic()
redwine<-redwine%>%mutate(quality_wine=as.factor(quality_wine))
typeof(redwine$quality_wine)
```

Above plot shows what we have inferred previously, that good wines were outnumbered by bad wines by a large margin. Most wines were mediocre (rated 5 or 6), but we could also see that there are some poor wines (3 or 4). A vast majority of good wines has a quality rating of 7.

```{r}
#Baseline Random Forest Model
redwineRF<-randomForest(factor(quality_wine)~.-quality,redwine,ntree=150)
redwineRF
```

The overall accuracy of our model is pretty good at around 88% overall. However, we could clearly see that it is much better in predicting bad wines than good ones.


```{r}
# for(i in 1:nrow(hyper_grid)) {
# 
#   # train model
#   model <- ranger(
#     formula         = quality_wine~ .,
#     data            = redwine,
#     num.trees       = 500,
#     mtry            = hyper_grid$mtry[i],
#     min.node.size   = hyper_grid$node_size[i],
#     sample.fraction = hyper_grid$sampe_size[i],
#     seed            = 0
#   )
# 
#   # add OOB error to grid
#   # hyper_grid$acc[i] <- (model$confusion.matrix[1,1]+ model$confusion.matrix[2,2]+ model$confusion.matrix[3,3] +model$confusion.matrix[4,4]+ model$confusion.matrix[5,5]+ model$confusion.matrix[6,6]) /  sum(model$confusion.matrix)
#   hyper_grid$acc[i] <- sum(diag(3)*model$confusion.matrix) /  sum(model$confusion.matrix)
# }
# 
# hyper_grid %>%
#   dplyr::arrange(acc) # %>%
#   # head(10)
```


