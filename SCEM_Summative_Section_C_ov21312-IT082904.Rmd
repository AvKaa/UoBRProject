---
title: "SCEM_Summative_Section_C_ov21312"
author: "Avekan"
date: "30/11/2021"
output: html_document
---

#Synopsis

As the publisher of this dataset suggests; due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).
This specific dataset can be viewed as classification or a regression task. The classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones).
As the publisher suggest, we will classify the wines by setting an arbitrary cutoff for our dependent variable (wine quality) at e.g. 7 or higher getting classified as 'good/1' and the remainder as 'not good/0'. For this specific kernel we will be doing some basic data explorations and will be doing classification utilizing a quick random forest as our baseline model. 

#Let's have a peek of our dataset

```{r, echo=FALSE}
# #Libraries needed
install.packages("tidyverse")
install.packages("corrplot")
install.packages("randomForest")
install.packages("ranger")
```

```{r}
# library(ggplot2, ggthemes, corrplot, reshape2, dplyr, randomForest)
library(tidyverse)
library(corrplot)
library(randomForest)
library(ranger)
```

```{r}
#Load red wine dataset
redwine<-read.csv("./winequality-red.csv")
redwine <- redwine %>%
  drop_na()
#summary statistics
str(redwine)
summary(redwine)
```

##Correlation of Variables

```{r,message=FALSE,warning=FALSE}
#Scatterplot Matrix of Variables
plot(redwine)

#Correlation Heatmap of Variables
corrplot(cor(redwine))
```

We're trying to predict wine quality, so we care about the final 2 columns/rows in order to know which among the variables has the strongest relationship with wine quality. As the heatmap suggests, alcohol has the strongest correlation with wine quality.

##Wine Quality

```{r,message=FALSE,warning=FALSE}
#Distribution of red wine quality ratings
ggplot(redwine,aes(x=quality))+geom_bar(stat = "count",position = "dodge")+
  scale_x_continuous(breaks = seq(3,8,1))+
  ggtitle("Distribution of Red Wine Quality Ratings")+
  theme_classic()

```


##Predictive Modelling (Binary Classification)

Random forest would be utilised as the baseline model in predicting the quality of a wine. Starting by splitting the data set into the train and test datasets.
```{r}
set.seed(0)

train_ratio <- 0.75
num_total <- redwine %>% nrow()
num_train <- floor(num_total*train_ratio)
num_test <- num_total-num_train

test_ind <- sample(seq(num_total),num_test)
train_ind <- setdiff(seq(num_total),test_ind)

redwine_train <- redwine %>% filter(row_number() %in% train_ind)
redwine_test <- redwine %>% filter(row_number() %in% test_ind)
```


```{r}
# RandomForest Model
set.seed(0) # Set seed for replicability 
redwine_model<-randomForest(factor(quality)~.-quality,redwine_train,ntree = 500, mtry = 6, importance = TRUE)

# Predicting on train set
pred_train <- predict(redwine_model, redwine_train, type='class')
# Checking classification accuracy
cm_train <- table(pred, redwine_train$quality) # Confusion matrix

# Predicting on test set
pred_test <- predict(redwine_model, redwine_test, type='class')
# Checking classification accuracy
accuracy <- mean(pred_test == redwine_test$quality)                    
cm_test <- table(pred_test,redwine_test$quality)

accuracy
cm_test
```

The overall accuracy of the model is around 70%. 

###Variable Importance

```{r,message=FALSE,warning=FALSE}
# Get importance
importance    <- randomForest::importance(redwineRF)

var_Importance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rank_Importance <- var_Importance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

# Use ggplot2 to visualize the relative importance of variables
ggplot(rank_Importance, aes(x = reorder(Variables, Importance), 
                           y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
            hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() + 
  theme_bw()
```

###Hyperparameter tuning
```{r}
# hyperparameter grid search
hyper_grid <- expand.grid(
  mtry       = seq(1, 10, by = 2),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  acc   = 0
)
```



```{r}
redwine<-redwine%>%mutate(quality=as.factor(quality)) # `quality` must be as factor

for(i in 1:nrow(hyper_grid)) {
  
  # train model
  model <- ranger(
    formula         = quality~ ., 
    data            = redwine_train, 
    num.trees       = 500,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed            = 0
  )
  
  # add confusion matrix to grid
  hyper_grid$acc[i] <- sum(diag(6)*model$confusion.matrix) /  sum(model$confusion.matrix)
}

hyper_grid %>%
  dplyr::arrange(desc(acc)) %>%
  head(10)
```


```{r}
#Load red wine dataset
redwine<-read.csv("./winequality-red.csv")
#Create a variable indicating if a wine is good or bad

quality_2_class<-function(quality){
  return(if(quality<5){
    as.integer(0)
  }else if(quality>6){
    as.integer(2)
  }else{
    as.integer(1)
  })
}


redwine$quality_wine<-map_dbl(.x=redwine$quality,.f=~quality_2_class(.x))
  

#Distribution of good/bad red wines
ggplot(redwine,aes(x=quality_wine,fill=factor(quality_wine)))+geom_bar(stat = "count",position = "dodge")+
  scale_x_continuous()+
  ggtitle("Distribution of Good/Mediocre/Bad Red Wines")+
  theme_classic()
redwine<-redwine%>%mutate(quality_wine=as.factor(quality_wine))
typeof(redwine$quality_wine)
```

Above plot shows what we have inferred previously, that good wines were outnumbered by bad wines by a large margin. Most wines were mediocre (rated 5 or 6), but we could also see that there are some poor wines (3 or 4). A vast majority of good wines has a quality rating of 7.

```{r}
#Baseline Random Forest Model
redwineRF<-randomForest(factor(quality_wine)~.-quality,redwine,ntree=150)
redwineRF
```

The overall accuracy of our model is pretty good at around 88% overall. However, we could clearly see that it is much better in predicting bad wines than good ones.


```{r}
for(i in 1:nrow(hyper_grid)) {

  # train model
  model <- ranger(
    formula         = quality_wine~ .,
    data            = redwine,
    num.trees       = 500,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed            = 0
  )

  # add OOB error to grid
  # hyper_grid$acc[i] <- (model$confusion.matrix[1,1]+ model$confusion.matrix[2,2]+ model$confusion.matrix[3,3] +model$confusion.matrix[4,4]+ model$confusion.matrix[5,5]+ model$confusion.matrix[6,6]) /  sum(model$confusion.matrix)
  hyper_grid$acc[i] <- sum(diag(3)*model$confusion.matrix) /  sum(model$confusion.matrix)
}

hyper_grid %>%
  dplyr::arrange(acc) # %>%
  # head(10)
```


